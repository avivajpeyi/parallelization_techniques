{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a target=\"_blank\" href=\"https://github.com/avivajpeyi/parallelization_techniques/blob/main/docs/basics.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Py Parallelization Basics\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vectorization\n",
    "Lets start with a simple vectorization example"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import process_time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "# not returning anything here, just doing the computation\n",
    "def fn(x):\n",
    "    val = x + x * x + x * x * x\n",
    "    pass\n",
    "\n",
    "\n",
    "def runtime_loops(x):\n",
    "    t0 = process_time()\n",
    "    rows, cols = x.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            fn(x[i, j])\n",
    "    return process_time() - t0\n",
    "\n",
    "\n",
    "def runtime_np_vectorized(x):\n",
    "    t0 = process_time()\n",
    "    fn(x)\n",
    "    return process_time() - t0\n",
    "\n",
    "N = 1000\n",
    "x = np.random.randn(N, N).astype(dtype='float32')\n",
    "print(f\"Loops: {runtime_loops(x):.3f}s\")\n",
    "print(f\"Vectorized: {runtime_np_vectorized(x):.3f}s\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multiprocessing and Multithreading\n",
    "\n",
    "For multiprocessing and multithreading, things will work better if you first _save_ the code to file, and then import the functions. This is because the multiprocessing module will need to import the code in a new process, and it will not be able to find the functions if they are defined in a notebook cell."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile basics_multi_demo.py\n",
    "\n",
    "import multiprocessing as mp\n",
    "from time import process_time\n",
    "\n",
    "N_CPUS = mp.cpu_count()\n",
    "\n",
    "def fn(x):\n",
    "    val = x + x * x + x * x * x\n",
    "    pass\n",
    "\n",
    "def runtime_multiprocessing(x, processes=N_CPUS):\n",
    "    pool = mp.Pool(processes=processes)\n",
    "    t0 = process_time()\n",
    "    pool.map(fn, x)\n",
    "    return process_time() - t0\n",
    "\n",
    "\n",
    "def runtime_multithreading(x,processes=N_CPUS):\n",
    "    pool = mp.pool.ThreadPool(processes=processes)\n",
    "    t0 = process_time()\n",
    "    pool.map(fn, x)\n",
    "    return process_time() - t0\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from basics_multi_demo import runtime_multiprocessing, runtime_multithreading\n",
    "\n",
    "print(f\"Loops: {runtime_loops(x):.3f}s\")\n",
    "print(f\"Multiprocessing: {runtime_multiprocessing(x):.3f}s\")\n",
    "print(f\"Multithreading: {runtime_multithreading(x):.3f}s\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Why is one slower?**\n",
    "\n",
    "Multithreading is more lightweight because most system and memory resources are shared by the threads. BUT there is overhead associated with managing threads, so you donâ€™t want to use it for basic tasks.\n",
    "\n",
    "In addition, the fact that multiple threads all access a shared pool of memory is extremely convenient for numerical programming.\n",
    "\n",
    "On the other hand, multiprocessing is more flexible and can be distributed across clusters.\n",
    "\n",
    "Generally multithreading is best suited for I/O-bound tasks, while multiprocessing is best suited for CPU-bound tasks.\n",
    "\n",
    "NOTE: On Colab, this may not work as expected...\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GPU + TPU\n",
    "\n",
    "Lets test two GPU, graphics processing unit, libraries, [CuPy] and [JAX] (JAX works with TPUs, tensor processing units as well).\n",
    "\n",
    "[CuPy]: https://docs.cupy.dev/en/stable/user_guide/basic.html\n",
    "[JAX]: https://jax.readthedocs.io/en/latest/notebooks/quickstart.html\n",
    "\n",
    "``` warning\n",
    "On Colab you may need to change your runntime to use a GPU:\n",
    "\n",
    "> Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    # Check the current device\n",
    "    print(f\"CuPy platform: {cp.cuda.Device()}\")\n",
    "    CUPY_INSTALLED = True\n",
    "except ImportError:\n",
    "    CUPY_INSTALLED = False\n",
    "    warnings.warn(\"Cupy not installed\")\n",
    "\n",
    "try:\n",
    "    import jax.numpy as jnp\n",
    "    from jax import jit\n",
    "    from jax.lib import xla_bridge\n",
    "    print(f\"JAX platform: {xla_bridge.get_backend().platform}\")\n",
    "    JAX_INSTALLED = True\n",
    "except ImportError:\n",
    "    JAX_INSTALLED = False\n",
    "    warnings.warn(\"Jax not installed\")\n",
    "\n",
    "\n",
    "if JAX_INSTALLED:\n",
    "  jax_fn = jit(fn)\n",
    "\n",
    "def runtime_jax(x):\n",
    "    if not JAX_INSTALLED:\n",
    "        return np.nan\n",
    "    x = jnp.array(x)\n",
    "    t0 = process_time()\n",
    "    jax_fn(x).block_until_ready()\n",
    "    return process_time() - t0\n",
    "\n",
    "\n",
    "def runtime_cupy(x):\n",
    "    if not CUPY_INSTALLED:\n",
    "        return np.nan\n",
    "    x = cp.array(x)\n",
    "    t0 = process_time()\n",
    "    fn(x)\n",
    "    return process_time() - t0\n",
    "\n",
    "\n",
    "print(f\"Loops: {runtime_loops(x):.3f}s\")\n",
    "print(f\"CuPy: {runtime_cupy(x):.3f}s\")\n",
    "print(f\"JAX: {runtime_jax(x):.3f}s\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lets make a runtime comparison plot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "RUNTIME_FUNCS = dict(\n",
    "    loops=runtime_loops,\n",
    "    np_vectorized=runtime_np_vectorized,\n",
    "    multiprocessing=runtime_multiprocessing,\n",
    "    multithreading=runtime_multithreading,\n",
    "    jax=runtime_jax,\n",
    "    cupy=runtime_cupy,\n",
    ")\n",
    "\n",
    "\n",
    "def collect_runtime_data(n_vals, n_trials=5):\n",
    "    runtimes = {k: [] for k in RUNTIME_FUNCS.keys()}\n",
    "    for i, n in tqdm(enumerate(n_vals), total=len(n_vals)):\n",
    "        x = np.random.randn(n, n).astype(dtype='float32')\n",
    "        for k, fn in RUNTIME_FUNCS.items():\n",
    "            trials = np.empty(n_trials)\n",
    "            for j in range(n_trials):\n",
    "                trials[j] = fn(x)\n",
    "            if n_trials > 1:\n",
    "                trials = trials[trials.argsort()[:-1]]\n",
    "            runtimes[k].append(np.quantile(trials, [0.05, 0.5, 0.95]))\n",
    "\n",
    "    for k in RUNTIME_FUNCS.keys():\n",
    "      runtimes[k] = np.array(runtimes[k])\n",
    "\n",
    "    return runtimes\n",
    "\n",
    "\n",
    "def plot_runtimes(n_vals, runtimes):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    for i, (k, v) in enumerate(runtimes.items()):\n",
    "        ax.plot(n_vals, v[:,1], label=k, color=f'C{i}')\n",
    "        ax.fill_between(n_vals, v[:,0], v[:,2], alpha=0.2, color=f'C{i}')\n",
    "    ax.set_xlabel('Array Size')\n",
    "    ax.set_ylabel('Runtime (s)')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlim(min(n_vals),max(n_vals))\n",
    "    ax.legend(fontsize=15, frameon=False)\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "n_vals = np.geomspace(1e2, 1e3, 10).astype(int)\n",
    "runtimes = collect_runtime_data(n_vals)\n",
    "plot_runtimes(n_vals, runtimes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](static/basics_runtime.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<script src=\"https://utteranc.es/client.js\"\n",
    "        repo=\"avivajpeyi/parallelization_techniques\"\n",
    "        issue-term=\"title\"\n",
    "        theme=\"github-light\"\n",
    "        crossorigin=\"anonymous\"\n",
    "        async>\n",
    "</script>"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
