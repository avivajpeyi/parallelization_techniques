{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# GPU: CuPy, Numba-GPU, PyCUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "conda install cupy cudatoolkit\n",
    "```\n",
    "\n",
    "```\n",
    "export CFLAGS=-fpermissive\n",
    "pip install --no-cache-dir pycuda     # I have more luck with this one in pip\n",
    "```\n",
    "\n",
    "_(and numpy, matplotlib, numba from before)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you can get better memory efficiency using rowwise code (e.g. compiled for loops), why would you ever write columnar code (e.g. Numpy)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Answer:** vectorization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Vectorization is a vertical scaling technique that uses a single CPU core or a GPU more effectively. You can compute N operations at the same time _if they are all the same operation._\n",
    "\n",
    "<center><img src=\"img/vectorization-example.png\" width=\"50%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you don't fully utilize all cores, that's okay; someone else's work can fill the gaps.\n",
    "\n",
    "If you don't fully utilize the core's vector unit, no one else can use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A GPU is a computational device designed around vector units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Like parallel processing, this is another computing detail that is visible to you as a data analyst.\n",
    "\n",
    "Rowwise code like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "@numba.jit\n",
    "def run_numba_loop(height, width, maxiterations, c, fractal):\n",
    "    for h in range(height):\n",
    "        for w in range(width):\n",
    "            z = c[h, w]\n",
    "            for i in range(maxiterations):\n",
    "                z = z**2 + c[h, w]\n",
    "                if abs(z) > 2:\n",
    "                    fractal[h, w] = i\n",
    "                    break\n",
    "    return fractal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "does not use vector units effectively because each array element may be in a different stage of processing— some may have diverged before others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Columnar code like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import time\n",
    "\n",
    "def prepare(height, width):\n",
    "    y, x = numpy.ogrid[-1:0:height*1j, -1.5:0:width*1j]\n",
    "    c = x + y*1j\n",
    "    fractal = numpy.zeros(c.shape, dtype=numpy.int32)\n",
    "    return c, fractal\n",
    "\n",
    "def run(c, fractal, maxiterations=20):\n",
    "    fractal *= 0                  # set fractal to maxiterations without replacing it\n",
    "    fractal += maxiterations\n",
    "    z = c\n",
    "    for i in range(maxiterations):\n",
    "        z = z**2 + c\n",
    "        diverge = z.real**2 + z.imag**2 > 2**2\n",
    "        divnow = diverge & (fractal == maxiterations)\n",
    "        fractal[divnow] = i\n",
    "        z[diverge] = 2\n",
    "    return fractal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can use vector units effectively because it's always applying the <b>S</b>ame <b>I</b>nstruction on <b>M</b>ultiple <b>D</b>ata (SIMD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "All we need is a librrary to implement the Numpy functions on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "c, fractal = prepare(4000, 6000)\n",
    "\n",
    "c = cupy.array(c)\n",
    "fractal = cupy.array(fractal)\n",
    "\n",
    "starttime = time.time()\n",
    "fractal = run(c, fractal)\n",
    "time.time() - starttime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "c, fractal = prepare(4000, 6000)\n",
    "\n",
    "starttime = time.time()\n",
    "fractal = run(c, fractal)\n",
    "time.time() - starttime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Exactly the same code: first with CuPy on the GPU (2.8 sec), then with Numpy on the CPU (7.5 sec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you're wondering why I'm working on a reduced problem (4× smaller than previous sessions), it's because I couldn't fit the full one in my GPU's memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(There's always a catch!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Also, CuPy's adherence to the Numpy API isn't perfect: I had to write\n",
    "\n",
    "```python\n",
    "z.real**2 + z.imag**2\n",
    "```\n",
    "\n",
    "instead of\n",
    "\n",
    "```python\n",
    "numpy.absolute(z)\n",
    "```\n",
    "\n",
    "because the `absolute` function wasn't supported. This is the error you'd see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    numpy.absolute(cupy.array([1.1, 2.2, 3.3]))\n",
    "except ValueError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Nevertheless, we can expect CuPy to become more complete as people use it and report missing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**GPU method #2:** Use Numba! (You have to install a \"cudatoolkit\" library with it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba.cuda\n",
    "import math\n",
    "\n",
    "@numba.cuda.jit\n",
    "def as_cuda(c, fractal, maxiterations):\n",
    "    x, y = numba.cuda.grid(2)     # 2 dimensional CUDA grid\n",
    "    z = c[x, y]\n",
    "    fractal[x, y] = 20\n",
    "    for i in range(maxiterations):\n",
    "        z = z**2 + c[x, y]\n",
    "        if abs(z) > 2:\n",
    "            fractal[x, y] = i\n",
    "            break                 # not optimal: threads that leave the loop still have to wait\n",
    "\n",
    "def run_numba(height, width, maxiterations=20):\n",
    "    y, x = numpy.ogrid[-1:0:height*1j, -1.5:0:width*1j]\n",
    "    c = x + y*1j\n",
    "    fractal = numba.cuda.device_array(c.shape, dtype=numpy.int32)\n",
    "    as_cuda[(math.ceil(height / 32), math.ceil(width / 32)), (32, 32)](c, fractal, maxiterations)\n",
    "    return fractal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = time.time()\n",
    "fractal = run_numba(4000, 6000)\n",
    "time.time() - starttime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On the same sized problem,\n",
    "\n",
    "   * Numpy on the CPU: 7.5 sec\n",
    "   * CuPy on the GPU: 2.8 sec\n",
    "   * Numba on the GPU: 0.3 sec\n",
    "\n",
    "And Numba doesn't suffer from the memory issue because it doesn't make as many intermediate copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "starttime = time.time()\n",
    "fractal = run_numba(8000, 12000)    # full-sized problem\n",
    "time.time() - starttime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That full-sized problem used to take us half a minute in Numpy, and (projected) 15 minutes in pure Python. For sanity's sake, we verify that it is, indeed, drawing our fractal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.imshow(fractal)\n",
    "# ax.imshow(fractal[-2000:, :3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Method #3:** PyCUDA. This library is somewhat older and hard to distribute nowadays (it's not Python 3.7 compliant). However, it is unique in letting you write any CUDA code (e.g. copied from the web) in Python without wrapping it as a library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycuda.autoinit\n",
    "import pycuda.driver\n",
    "import pycuda.compiler\n",
    "\n",
    "module = pycuda.compiler.SourceModule(\"\"\"\n",
    "__global__ void from_pycuda(double* c, int* fractal, int height, int width, int maxiterations) {\n",
    "    const int x = threadIdx.x + blockIdx.x*blockDim.x;\n",
    "    const int y = threadIdx.y + blockIdx.y*blockDim.y;\n",
    "    double creal = c[2 * (x + height*y)];\n",
    "    double cimag = c[2 * (x + height*y) + 1];\n",
    "    double zreal = creal;\n",
    "    double zimag = cimag;\n",
    "    fractal[x + height*y] = maxiterations;\n",
    "    for (int i = 0;  i < maxiterations;  i++) {\n",
    "        double zreal2 = zreal*zreal + zimag*zimag + creal;\n",
    "        double zimag2 = zreal*zreal + zimag*zimag + cimag;\n",
    "        zreal = zreal2;\n",
    "        zimag = zimag2;\n",
    "        if (zreal*zreal + zimag*zimag > 4) {\n",
    "            fractal[x + height*y] = i;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\")\n",
    "from_pycuda = module.get_function(\"from_pycuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def run_pycuda(height, width, maxiterations=20):\n",
    "    y, x = numpy.ogrid[-1:0:height*1j, -1.5:0:width*1j]\n",
    "    c = x + y*1j\n",
    "    fractal = numpy.empty(c.shape, dtype=numpy.int32) + maxiterations\n",
    "    from_pycuda(pycuda.driver.In(c.view(numpy.float64)),\n",
    "                pycuda.driver.Out(fractal),\n",
    "                numpy.int32(height),\n",
    "                numpy.int32(width),\n",
    "                numpy.int32(maxiterations),\n",
    "                block=(32, 32, 1),\n",
    "                grid=(int(math.ceil(height / 32)), int(math.ceil(width / 32))))\n",
    "    return fractal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = time.time()\n",
    "fractal = run_pycuda(8000, 12000)    # full-sized problem\n",
    "time.time() - starttime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "About the same as Numba (10% better), which wouldn't be worth it for having to translate Python into CUDA C++, but would be worth it if you _found_ CUDA C++ and didn't want to translate it into Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As before, each library has its own special niche:\n",
    "\n",
    "   * **CuPy:** for directly running Numpy on GPUs, no questions asked\n",
    "   * **Numba:** for running (a limited subset of) Python code directly on the GPU\n",
    "   * **PyCUDA:** for running CUDA C++ with the convenience of Numpy input and output."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
